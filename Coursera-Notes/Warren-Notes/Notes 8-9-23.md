# Notes 8-9-23

## Week 4 of Course 1


* **Neural Networks** with more layers are called **Deep Neural Networks**

* Certain problems can be solved only by **Deep Neural Networks**.

* The process of forward propagation for a **Deep Neural Network** is similar to that of a single layer.

* The activation functions of the previous layer are fed to the next layer as input.

* **Hyperparameters** are parameters whose values are set before training.

* Unlike regular parameters like weights and biases, **hyperparameters** control the learning process and influence the weights and biases learned by the model.

* Examples of hyperparameters include, *learning_rate*, *number of hidden layers*, *number of units per layer*, *choice of activation function*, etc.

* These hyperparameters are usually empirically determined by seeing which value yields the most optimum result.

* The earlier layers in a **Deep Neural Network** usually capture some of the simpler, smaller relations between the inputs.

* Whereas, the latter layers capture the more complex relations between the observations of the earlier layers.

* This enables **Deep Neural Networks** to perform more complex tasks, with better accuracy than **Shallow Neural Networks**

* In general, a **Shallow Neural Network** of say 1 hidden layer, would require exponentially more hidden units to keep up with a **Deep Neural Network** having 4-5 layers with lesser units per layer.

# To add: Equations for N layer forward and back propagation.