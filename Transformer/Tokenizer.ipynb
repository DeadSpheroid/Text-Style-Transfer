{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc99nShk436-",
        "outputId": "367f49f0-2784-4ac2-e11f-a174c0ba0036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \"tensorflow-text==2.11.*\"\n",
        "!pip install -q tensorflow_datasets\n",
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFZF9Wb05VEO",
        "outputId": "5cd40c9f-6dfb-457f-8a25-318720613a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ExJ0V364514J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/final.csv')\n",
        "# x = df['og']\n",
        "df = df.drop(['Unnamed: 0','id'], axis=1)\n",
        "# print(df.head())\n",
        "# for col in df.columns:\n",
        "#     print(col)\n",
        "# y = df['t']\n",
        "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
        "# print(train)\n",
        "# print(test)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
        "# print(test_ds)\n",
        "# print(train_ds)\n",
        "def map_func(x):\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "  # print(x1, x2)\n",
        "  return (x1,x2,)\n",
        "train_ds = train_ds.map(map_func)\n",
        "test_ds = test_ds.map(map_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV_l361r5WEB",
        "outputId": "ff842000-4a20-41c6-8130-51bf592dfdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(), dtype=string) Tensor(\"strided_slice_1:0\", shape=(), dtype=string)\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=string) Tensor(\"strided_slice_1:0\", shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_en = train_ds.map(lambda shk, en: en)\n",
        "train_shk = train_ds.map(lambda shk, en: shk)"
      ],
      "metadata": {
        "id": "pDz0KDjl5zPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "9dNZCdAI6JT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 80_000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")\n"
      ],
      "metadata": {
        "id": "LEGLnIO16NL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "shk_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_shk.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9yY89Fc6X1r",
        "outputId": "24314b39-24d8-4a9d-be7f-e9cbc153e056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 10s, sys: 712 ms, total: 1min 10s\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(shk_vocab[:10])\n",
        "print(shk_vocab[100:110])\n",
        "print(shk_vocab[1000:1010])\n",
        "print(shk_vocab[-10:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f15iFvdD6bcm",
        "outputId": "5c23d059-4279-4e2a-92a9-8be8a0cd5de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '&', \"'\", '(', ')']\n",
            "['from', 'sir', '##ed', 'come', 'll', 'she', 'at', 'they', 'or', 'which']\n",
            "['##ge', 'choose', 'commend', 'judge', 'laugh', 'order', 'patient', 'roman', 'wild', 'year']\n",
            "['##]', '##j', '##q', '##æ', '##–', '##—', '##‘', '##’', '##“', '##”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)\n"
      ],
      "metadata": {
        "id": "UIuQKEQj6y8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_vocab_file('drive/MyDrive/shk_vocab.txt', shk_vocab)\n"
      ],
      "metadata": {
        "id": "bKmSlD7m63Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAOg68j96_Fm",
        "outputId": "600e8e80-b95c-4e89-e45b-97160706ea53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 45.8 s, sys: 840 ms, total: 46.6 s\n",
            "Wall time: 46.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-5YQBKY7EVx",
        "outputId": "0a5df228-efb6-4e82-8ce0-de400c28984b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', \"'\", '(', ')']\n",
            "['by', 'no', 'now', 'good', 'would', 'she', 'here', 'when', 'don', 'then']\n",
            "['rude', 'takes', 'wedding', '##man', 'choose', 'earl', 'enjoy', 'friar', 'heads', 'learn']\n",
            "['##]', '##j', '##q', '##v', '##–', '##—', '##‘', '##’', '##“', '##”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_vocab_file('drive/MyDrive/en_vocab.txt', en_vocab)\n"
      ],
      "metadata": {
        "id": "v2VgUQ237ULG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shk_tokenizer = text.BertTokenizer('drive/MyDrive/shk_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer('drive/MyDrive/en_vocab.txt', **bert_tokenizer_params)\n"
      ],
      "metadata": {
        "id": "GpvFO9M27Z1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)\n"
      ],
      "metadata": {
        "id": "PaPKeee29cT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "fNT40KuU90YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:\n",
        "\n",
        "    # Include a tokenize signature for a batch of strings.\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)\n"
      ],
      "metadata": {
        "id": "aqv_jV2T92td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.shk = CustomTokenizer(reserved_tokens, 'drive/MyDrive/shk_vocab.txt')\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, 'drive/MyDrive/en_vocab.txt')"
      ],
      "metadata": {
        "id": "MEKBxHr996qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'drive/MyDrive/shk_to_en_tokenizers'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "yA03EhB2-EGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r {model_name}.zip {model_name}\n",
        "!du -h drive/MyDrive/shk_to_en_tokenizers.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ5gquBm_ZIC",
        "outputId": "86484e44-a22e-4299-c983-f3d9a0b7ce24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: drive/MyDrive/shk_to_en_tokenizers/ (stored 0%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/variables/ (stored 0%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/variables/variables.data-00000-of-00001 (deflated 50%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/variables/variables.index (deflated 33%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/assets/ (stored 0%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/assets/en_vocab.txt (deflated 53%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/assets/shk_vocab.txt (deflated 52%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/fingerprint.pb (stored 0%)\n",
            "updating: drive/MyDrive/shk_to_en_tokenizers/saved_model.pb (deflated 91%)\n",
            "142K\tdrive/MyDrive/shk_to_en_tokenizers.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "InlmsB3a_jma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}