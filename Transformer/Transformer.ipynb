{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLulCi-vR_ph",
        "outputId": "1d204348-da84-4b86-a48f-9743d307f852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libcudnn8 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mVersion '8.1.0.77-1+cuda11.2' for 'libcudnn8' was not found\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: protobuf~=3.20.3 in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1hRAv7UTUZ2"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU5FaFIlTksZ",
        "outputId": "28d40531-b286-4e68-e6bb-b72967a9c38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nBnDy3XR_pj"
      },
      "outputs": [],
      "source": [
        "model_name = 'drive/MyDrive/shk_to_en_tokenizers'\n",
        "# tf.keras.utils.get_file(\n",
        "#     f'{model_name}.zip',\n",
        "#     f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "#     cache_dir='.', cache_subdir='', extract=True\n",
        "# )\n",
        "tokenizers = tf.saved_model.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6PjDvH8R_pk",
        "outputId": "81fa0880-3f7d-4aaa-e1c5-48800919d65d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(), dtype=string) Tensor(\"strided_slice_1:0\", shape=(), dtype=string)\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=string) Tensor(\"strided_slice_1:0\", shape=(), dtype=string)\n",
            "**** <_MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n",
            "!!!!! (b' When day\\xe2\\x80\\x99s oppression is not eased by night,  But day by night and night by day oppressed? ', b'When the pressure of the day is not relieved at night,But day is oppressed by night and night by day?') <class 'tuple'>\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('drive/MyDrive/final.csv')\n",
        "# x = df['og']\n",
        "df = df.drop(['Unnamed: 0','id'], axis=1)\n",
        "# print(df.head())\n",
        "# for col in df.columns:\n",
        "#     print(col)\n",
        "# y = df['t']\n",
        "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
        "# print(train)\n",
        "# print(test)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
        "# print(test_ds)\n",
        "# print(train_ds)\n",
        "def map_func(x):\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "  print(x1, x2)\n",
        "  return (x1,x2,)\n",
        "train_ds = train_ds.map(map_func)\n",
        "test_ds = test_ds.map(map_func)\n",
        "print(\"****\",train_ds)\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
        "itr_tf = train_ds.as_numpy_iterator()\n",
        "for ele in itr_tf:\n",
        "    print(\"!!!!!\", ele, type(ele))\n",
        "    break\n",
        "# for sh_examples, en_examples in train_ds.batch(3).take(1):\n",
        "#   print('> Examples in Shakespeare:')\n",
        "#   for sh in sh_examples.numpy():\n",
        "#     print(sh)\n",
        "#   print()\n",
        "\n",
        "#   print('> Examples in English:')\n",
        "#   for en in en_examples.numpy():\n",
        "#     print(en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f7d7MdyR_pm",
        "outputId": "4e117d22-6fc3-4001-c249-44298dbc40c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5638 5968\n"
          ]
        }
      ],
      "source": [
        "MAX_TOKENS=128\n",
        "BATCH_SIZE=64\n",
        "BUFFER_SIZE=20_000\n",
        "def prepare_batch(shk,en):\n",
        "\n",
        "    shk =tokenizers.shk.tokenize(shk)     # Output is ragged.\n",
        "    shk = shk[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
        "    shk = shk.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en[:, :(MAX_TOKENS+1)]\n",
        "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (shk, en_inputs), en_labels\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "  )\n",
        "\n",
        "print(tokenizers.en.get_vocab_size().numpy(), tokenizers.shk.get_vocab_size().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYJmkopxR_pm"
      },
      "outputs": [],
      "source": [
        "train_batches = make_batches(train_ds)\n",
        "val_batches = make_batches(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmwwqUT3a4ax",
        "outputId": "93fc698c-b268-4d42-bc5d-66c40eda78c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 56)\n",
            "(64, 59)\n",
            "(64, 59)\n"
          ]
        }
      ],
      "source": [
        "for (shk, en), en_labels in train_batches.take(1):\n",
        "  break\n",
        "\n",
        "print(shk.shape)\n",
        "print(en.shape)\n",
        "print(en_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "804YDlHQR_pn",
        "outputId": "b0347b02-b8ca-4e4f-be5f-0b022db6095b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((array([[   2,   64,   53, ...,    0,    0,    0],\n",
            "       [   2,   23, 1451, ...,    0,    0,    0],\n",
            "       [   2,   75, 2758, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,   83, 2390, ...,    0,    0,    0],\n",
            "       [   2,  110,  220, ...,    0,    0,    0],\n",
            "       [   2,   57,   31, ...,    0,    0,    0]]), array([[  2,  83,  60, ...,   0,   0,   0],\n",
            "       [  2, 101, 117, ...,   0,   0,   0],\n",
            "       [  2,  85, 117, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [  2, 177,  96, ...,   0,   0,   0],\n",
            "       [  2, 106,   7, ...,   0,   0,   0],\n",
            "       [  2,  62,  35, ...,   0,   0,   0]])), array([[  83,   60,  303, ...,    0,    0,    0],\n",
            "       [ 101,  117,   99, ...,    0,    0,    0],\n",
            "       [  85,  117, 2850, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 177,   96,   83, ...,    0,    0,    0],\n",
            "       [ 106,    7,   45, ...,    0,    0,    0],\n",
            "       [  62,   35,  178, ...,    0,    0,    0]]))\n"
          ]
        }
      ],
      "source": [
        "for ele in train_batches.take(1).as_numpy_iterator():\n",
        "    print(ele)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqPI38DiR_po"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO9RzfJOR_pp"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):#encoding + embedding\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8uxak-4R_pp"
      },
      "outputs": [],
      "source": [
        "# embed_shk = PositionalEmbedding(vocab_size=40_000 + 1, d_model=16)\n",
        "# embed_en = PositionalEmbedding(vocab_size=40_000 + 1, d_model=16)\n",
        "\n",
        "# shk_emb = embed_shk(vec_x_train)\n",
        "# en_emb = embed_en(vec_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7OFi_VdR_pr"
      },
      "outputs": [],
      "source": [
        "# en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO0LOvYIR_pr"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c8xGJqqR_ps"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS575wZHR_ps"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnvwwYJyR_pt"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF9YeMRRR_pt"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GiQ53o5R_pt"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3WSzb1BR_pu"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYvRoG9SR_pu"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uoclidbR_pv"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF6QhjKIR_pv"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    # print(list(inputs.as_numpy_iterator()))\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-afGtOPR_pv"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=4,\n",
        "    d_model=256,\n",
        "    num_heads=8,\n",
        "    dff=512,\n",
        "    input_vocab_size=tokenizers.shk.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "    dropout_rate=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfmJ64SFR_pw"
      },
      "outputs": [],
      "source": [
        "# example = \"\"\n",
        "# for batch in val_batches:\n",
        "#     # print(batch, \"***********************\")\n",
        "#     example = batch\n",
        "#     print(example)\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPtuJyStR_pw"
      },
      "outputs": [],
      "source": [
        "# transformer.build((2,100))\n",
        "output = transformer((shk,en))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6kPpF41R_px",
        "outputId": "aabad970-77c0-44c6-fa55-80f6b7f913ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  10997760  \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  19329536  \n",
            "                                                                 \n",
            " dense_16 (Dense)            multiple                  1448966   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31776262 (121.22 MB)\n",
            "Trainable params: 31776262 (121.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnjdu0AJR_px"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model=128, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(256)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWTfGNwwR_py"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rezDTO8R_py",
        "outputId": "02c73a83-3238-4a4e-eb7f-601f7cad4078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  10997760  \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  19329536  \n",
            "                                                                 \n",
            " dense_16 (Dense)            multiple                  1448966   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31776262 (121.22 MB)\n",
            "Trainable params: 31776262 (121.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0F48juiR_pz",
        "outputId": "5f2f371b-4d25-467b-d2c5-0268de2fa38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "729/729 [==============================] - 311s 425ms/step - loss: 4.5711 - masked_accuracy: 0.2721 - val_loss: 4.0890 - val_masked_accuracy: 0.3253\n",
            "Epoch 2/25\n",
            "729/729 [==============================] - 306s 419ms/step - loss: 3.9385 - masked_accuracy: 0.3364 - val_loss: 3.6222 - val_masked_accuracy: 0.3776\n",
            "Epoch 3/25\n",
            "729/729 [==============================] - 305s 418ms/step - loss: 3.5471 - masked_accuracy: 0.3781 - val_loss: 3.3729 - val_masked_accuracy: 0.4063\n",
            "Epoch 4/25\n",
            "729/729 [==============================] - 306s 419ms/step - loss: 3.2840 - masked_accuracy: 0.4067 - val_loss: 3.1413 - val_masked_accuracy: 0.4351\n",
            "Epoch 5/25\n",
            "729/729 [==============================] - 307s 420ms/step - loss: 3.1120 - masked_accuracy: 0.4245 - val_loss: 3.0190 - val_masked_accuracy: 0.4512\n",
            "Epoch 6/25\n",
            "729/729 [==============================] - 305s 418ms/step - loss: 2.9096 - masked_accuracy: 0.4480 - val_loss: 2.9031 - val_masked_accuracy: 0.4656\n",
            "Epoch 7/25\n",
            "729/729 [==============================] - 297s 406ms/step - loss: 2.7201 - masked_accuracy: 0.4700 - val_loss: 2.7693 - val_masked_accuracy: 0.4870\n",
            "Epoch 8/25\n",
            "729/729 [==============================] - 306s 418ms/step - loss: 2.5680 - masked_accuracy: 0.4891 - val_loss: 2.7078 - val_masked_accuracy: 0.4942\n",
            "Epoch 9/25\n",
            "729/729 [==============================] - 305s 417ms/step - loss: 2.4412 - masked_accuracy: 0.5046 - val_loss: 2.6655 - val_masked_accuracy: 0.5014\n",
            "Epoch 10/25\n",
            "729/729 [==============================] - 306s 419ms/step - loss: 2.3340 - masked_accuracy: 0.5176 - val_loss: 2.6236 - val_masked_accuracy: 0.5069\n",
            "Epoch 11/25\n",
            "729/729 [==============================] - 304s 417ms/step - loss: 2.2381 - masked_accuracy: 0.5306 - val_loss: 2.6087 - val_masked_accuracy: 0.5128\n",
            "Epoch 12/25\n",
            "729/729 [==============================] - 305s 418ms/step - loss: 2.1562 - masked_accuracy: 0.5412 - val_loss: 2.6056 - val_masked_accuracy: 0.5160\n",
            "Epoch 13/25\n",
            "729/729 [==============================] - 305s 418ms/step - loss: 2.0770 - masked_accuracy: 0.5513 - val_loss: 2.5915 - val_masked_accuracy: 0.5175\n",
            "Epoch 14/25\n",
            "729/729 [==============================] - 305s 418ms/step - loss: 2.0026 - masked_accuracy: 0.5612 - val_loss: 2.5890 - val_masked_accuracy: 0.5191\n",
            "Epoch 15/25\n",
            "729/729 [==============================] - 305s 418ms/step - loss: 1.9387 - masked_accuracy: 0.5701 - val_loss: 2.5955 - val_masked_accuracy: 0.5203\n",
            "Epoch 16/25\n",
            "729/729 [==============================] - 306s 420ms/step - loss: 1.8767 - masked_accuracy: 0.5790 - val_loss: 2.6089 - val_masked_accuracy: 0.5212\n",
            "Epoch 17/25\n",
            "729/729 [==============================] - 306s 419ms/step - loss: 1.8181 - masked_accuracy: 0.5872 - val_loss: 2.6174 - val_masked_accuracy: 0.5223\n",
            "Epoch 18/25\n",
            "729/729 [==============================] - 308s 421ms/step - loss: 1.7621 - masked_accuracy: 0.5956 - val_loss: 2.6394 - val_masked_accuracy: 0.5190\n",
            "Epoch 19/25\n",
            "729/729 [==============================] - 307s 420ms/step - loss: 1.7125 - masked_accuracy: 0.6027 - val_loss: 2.6538 - val_masked_accuracy: 0.5211\n",
            "Epoch 20/25\n",
            "729/729 [==============================] - 307s 420ms/step - loss: 1.6638 - masked_accuracy: 0.6103 - val_loss: 2.6594 - val_masked_accuracy: 0.5214\n",
            "Epoch 21/25\n",
            "729/729 [==============================] - 307s 420ms/step - loss: 1.6184 - masked_accuracy: 0.6173 - val_loss: 2.6832 - val_masked_accuracy: 0.5199\n",
            "Epoch 22/25\n",
            " 84/729 [==>...........................] - ETA: 4:20 - loss: 1.4894 - masked_accuracy: 0.6398"
          ]
        }
      ],
      "source": [
        "transformer.fit(train_batches,\n",
        "                epochs=25,\n",
        "                validation_data=val_batches)\n",
        "# 20 + 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbAf80LBR_pz"
      },
      "outputs": [],
      "source": [
        "transformer.save_weights('drive/MyDrive/my_model/transformer-256-16')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hje3PEl9Y7j"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.shk.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the\n",
        "    # English `[START]` token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "    return text, tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t53RuGJ9tRv"
      },
      "outputs": [],
      "source": [
        "translator = Translator(tokenizers, transformer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZMxq6Aw9wEH"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVE7tLwp9yYL",
        "outputId": "da821c1b-5c56-45cf-f550-7e26f51270c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:         : None but the king?\n",
            "Prediction     : nothing but the king ?\n",
            "Ground truth   : Only the king?\n"
          ]
        }
      ],
      "source": [
        "sentence = 'None but the king?'\n",
        "ground_truth = 'Only the king?'\n",
        "\n",
        "translated_text, translated_tokens = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waB62sqi-RCZ",
        "outputId": "be256ef1-c6fd-4464-c570-05bd951d86b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:         :  A sample to the youngest, to the more mature  A glass that feated them, and to the graver  A child that guided dotards; \n",
            "Prediction     : a sam - faced to the youngest daughter , to the more appropriate mirror that gave birth to a child who slapseed a child .\n",
            "Ground truth   : He was an example to the youngest, to the full-grown a model for their own behavior, and seemed to serious observers like a child leading old people.\n"
          ]
        }
      ],
      "source": [
        "sentence = ' A sample to the youngest, to the more mature  A glass that feated them, and to the graver  A child that guided dotards; '\n",
        "ground_truth = 'He was an example to the youngest, to the full-grown a model for their own behavior, and seemed to serious observers like a child leading old people.'\n",
        "\n",
        "translated_text, translated_tokens = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuHn8H32-tOu",
        "outputId": "a6249c9d-3c6e-464f-e246-3eb5008e2116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:         : For you, Posthumus,  So soon as I can win the offended king,  I will be known your advocate: \n",
            "Prediction     : you , posthumus , as soon as i can win the offend king , i will be known as your servant .\n",
            "Ground truth   : As for you, Posthumus, as soon as I can calm the upset king, I will speak in your defense.\n"
          ]
        }
      ],
      "source": [
        "sentence = 'For you, Posthumus,  So soon as I can win the offended king,  I will be known your advocate: '\n",
        "ground_truth = 'As for you, Posthumus, as soon as I can calm the upset king, I will speak in your defense.'\n",
        "\n",
        "translated_text, translated_tokens = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpeytNpD40w1",
        "outputId": "d93f6367-fe2d-4469-e71b-45f93d4c6903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:         : And as for thee my king, i pray that thou may be victorious in thy attempts\n",
            "Prediction     : and as for you , my king , i beg that you can be victorious in your attempt .\n",
            "Ground truth   : \n"
          ]
        }
      ],
      "source": [
        "sentence = 'And as for thee my king, i pray that thou may be victorious in thy attempts'\n",
        "ground_truth = \"\"\n",
        "\n",
        "translated_text, translated_tokens = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
