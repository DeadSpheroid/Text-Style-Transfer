{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db046b3-2b42-4d97-88a9-cc6bec3c9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def initialise_params(n_h,n_prev):\n",
    "    #He initialisation\n",
    "    W = np.random.randn(n_h,n_prev) * math.sqrt(2/n_prev)\n",
    "    \n",
    "    b = np.zeros((n_h,1))\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "def Leaky_ReLU(x):\n",
    "    return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_hidden, n_prev, activation = \"relu\"):\n",
    "        \n",
    "        self.n_h = n_hidden\n",
    "        \n",
    "        self.W , self.b = initialise_params(self.n_h,n_prev)\n",
    "        \n",
    "        self.activation_func = activation\n",
    "\n",
    "        self.grads = tuple()\n",
    "\n",
    "        self.layer_cache = tuple()\n",
    "\n",
    "    # Computes forward pass through this layer\n",
    "    def forward(self, A_prev):\n",
    "        \n",
    "        Z = np.dot(self.W,A_prev) + self.b\n",
    "\n",
    "        A = 0\n",
    "        \n",
    "        if self.activation_func == \"relu\":\n",
    "            A = Leaky_ReLU(Z)\n",
    "\n",
    "        elif self.activation_func == \"tanh\":\n",
    "            A = np.tanh(Z)\n",
    "\n",
    "        elif self.activation_func == \"sigmoid\":\n",
    "            A = sigmoid(Z)\n",
    "        \n",
    "        self.layer_cache = (Z,A,)\n",
    "\n",
    "        return A\n",
    "\n",
    "    # Computes backward pass through this layer\n",
    "    def backward(self, dA, A_prev):\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        dZ = 0\n",
    "        \n",
    "        Z = self.layer_cache[0]\n",
    "\n",
    "        if self.activation_func == \"relu\":\n",
    "            dZ = dA * np.where(Z > 0, 1, 0.01)\n",
    "\n",
    "        if self.activation_func == \"tanh\":\n",
    "            \n",
    "            dZ = dA * (1 - (np.tanh(Z) ** 2))\n",
    "\n",
    "        if self.activation_func == \"sigmoid\":\n",
    "\n",
    "            dZ = dA * (sigmoid(Z) * (1 - sigmoid(Z)))\n",
    "\n",
    "        dW = (1/m)*(np.dot(dZ, A_prev.T))\n",
    "\n",
    "        db = (1/m)*(np.sum(dZ, axis=1, keepdims=True))\n",
    "              \n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "               \n",
    "        self.grads = (dW, db, dZ, dA,)\n",
    "        \n",
    "        return dA_prev\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    L - layer model with each layer having adjustable number of  units\n",
    "    Only one output possible\n",
    "    Cross Entropy cost\n",
    "    tanh and sigmoid activations used, relu also available\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, layers_dims, activation_funcs):\n",
    "        \n",
    "        self.n_layers = len(layers_dims)\n",
    "\n",
    "        # List of layers of the model\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(1,self.n_layers):\n",
    "            \n",
    "            self.layers.append(Layer(layers_dims[i], layers_dims[i-1], activation_funcs[i-1]))\n",
    "\n",
    "        self.layers.append(Layer(1,layers_dims[i], \"sigmoid\")) # Final output layer\n",
    "\n",
    "\n",
    "\n",
    "    # Computes forward pass of model        \n",
    "    def forward_model(self, X):\n",
    "        \n",
    "        A = self.layers[0].forward(X)\n",
    "            \n",
    "        for i in range(1,len(self.layers)):\n",
    "            \n",
    "            A = self.layers[i].forward(A)\n",
    "\n",
    "        return A\n",
    "\n",
    "    # Computes backward pass of model\n",
    "    def backward_model(self, X, Y, AL):\n",
    "        \n",
    "        dA = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        \n",
    "\n",
    "        for i in reversed(range(1,len(self.layers))):\n",
    "\n",
    "            dA = self.layers[i].backward(dA, self.layers[i-1].layer_cache[1])\n",
    "\n",
    "\n",
    "        dA = self.layers[0].backward(dA, X)\n",
    "\n",
    "    # Perform gradient descent\n",
    "    def update_params(self, learning_rate = 0.05):\n",
    "        for layer in self.layers:\n",
    "\n",
    "            layer.W = layer.W - (learning_rate * layer.grads[0])\n",
    "            \n",
    "            layer.b = layer.b - (learning_rate * layer.grads[1])\n",
    "\n",
    "    \n",
    "    def train(self, X_train, Y_train, iterations = 1000, learning_rate = 0.05):\n",
    "\n",
    "        # Take one step of gradient descent per iteration\n",
    "        for i in range(1, iterations+1):\n",
    "            AL = self.forward_model(X_train)\n",
    "\n",
    "            self.backward_model(X_train,Y_train,AL)\n",
    "\n",
    "            self.update_params(learning_rate)\n",
    "\n",
    "            if i%10 == 0:\n",
    "                cost = self.compute_cost(AL, Y_train)\n",
    "                print(f\"Cost at iteration {i} is \", cost)\n",
    "\n",
    "    # Computes cross entropy cost\n",
    "    def compute_cost(self, ypred, y):\n",
    "        Y = y.reshape(1,y.shape[0])\n",
    "\n",
    "        m = Y.shape[1] # number of training examples\n",
    "        \n",
    "        logprobs = (np.multiply(np.log(ypred),y)) + (np.multiply(np.log(1-ypred),1-y))\n",
    "        \n",
    "        cost = -(1/m) * np.sum(logprobs)\n",
    "        \n",
    "        cost = float(np.squeeze(cost)) # Ensures that 12.35 is returned as opposed to [[12.35]]\n",
    "\n",
    "        return cost\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        A = self.forward_model(X_test)\n",
    "\n",
    "        cost = self.compute_cost(A, Y_test)\n",
    "\n",
    "        print(\"Accuracy on test set is: \", (1 - cost) * 100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e3c465-11c0-4cad-bbaf-073b510e169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3519fd-011a-462d-944b-55db55016fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv(\"./data_banknote_authentication.csv\")\n",
    "X_bank = pd.concat([bank_data[\"Attr1\"],bank_data[\"Attr2\"],bank_data[\"Attr3\"],bank_data[\"Attr4\"]], axis = 1)\n",
    "Y_bank = bank_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4273df-dd61-45f4-8251-2e69822e6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_bank, Y_bank, test_size=0.2)\n",
    "X_train_array = X_train.to_numpy().T\n",
    "Y_train_array = Y_train.to_numpy().T\n",
    "X_test_array = X_test.to_numpy().T\n",
    "Y_test_array = Y_test.to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b327fc9a-e4b5-452f-b089-a10f7537b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 at index 0 is the number of input features\n",
    "# 6,7,4 are the number of units in the 1, 2, 3rd layers\n",
    "layers_dims = [4,32,32,8,]\n",
    "activations = [\"relu\", \"tanh\", \"relu\",]\n",
    "DeepNN = Model(layers_dims, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8342c4-9c71-4372-a2f4-f7a0fb6e6e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 10 is  0.39828025236458914\n",
      "Cost at iteration 20 is  0.20616919747564386\n",
      "Cost at iteration 30 is  0.12327593878123255\n",
      "Cost at iteration 40 is  0.0808872807054035\n",
      "Cost at iteration 50 is  0.057458719848715464\n",
      "Cost at iteration 60 is  0.04357815365708165\n",
      "Cost at iteration 70 is  0.034501237569183885\n",
      "Cost at iteration 80 is  0.02816275217657873\n",
      "Cost at iteration 90 is  0.02350456350417371\n",
      "Cost at iteration 100 is  0.019973283312060316\n",
      "Cost at iteration 110 is  0.017226313189122395\n",
      "Cost at iteration 120 is  0.015071972724693168\n",
      "Cost at iteration 130 is  0.013344010609645966\n",
      "Cost at iteration 140 is  0.011929768749921762\n",
      "Cost at iteration 150 is  0.010755900326575448\n"
     ]
    }
   ],
   "source": [
    "DeepNN.train(X_train_array, Y_train_array, 150, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7e21a2-0be2-4b7c-a265-d6cb93d62845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is:  98.82822786267155 %\n"
     ]
    }
   ],
   "source": [
    "DeepNN.evaluate(X_test_array, Y_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f04122-2975-465f-8826-f5ee5b2631e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
